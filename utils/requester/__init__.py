#!/usr/bin/env python2.7
# -*- coding: utf-8 -*-


import traceback
import re
import urllib.request
import urllib.parse
import urllib.error
import logging
from os import path, makedirs, listdir, remove, stat, environ
from os.path import isdir, getctime
from json import loads, dumps, load
from http.cookiejar import MozillaCookieJar
from sys import stderr
from gzip import GzipFile
from hashlib import md5
from io import BytesIO
from time import sleep
from datetime import datetime
from string import ascii_letters, digits
from random import choice, gauss
from distutils.util import strtobool

import chardet


logging.getLogger('chardet.charsetprober').setLevel(logging.INFO)


class FileWithProxiesNotFound(Exception):
    pass


class ListProxiesEmpty(Exception):
    pass


class FailOnGetResponse(Exception):
    pass


class NoVerifyWord(Exception):
    pass


class SlowlyProxy(Exception):
    pass


class proxer():
    LIMIT_FAIL = 3
    DIVIDER = 3
    LIMIT_TIME = 2.5

    def load_data(self):
        try:
            with open(self.file_name, "r") as fo:
                self.data = load(fo)
        except (IOError, ValueError):
            self.data = {}

    def save_data(self):
        if (
            self.proxy and
            self.proxy["_fail"] >= self.LIMIT_FAIL
            or
            self.is_slow_proxy()
        ):
            del self.data[self.proxy_key]
        j = dumps(
            self.data,
            indent=4,
            sort_keys=True,
            ensure_ascii=False
        )
        with open(self.file_name, "w") as fo:
            fo.write(j)

    def is_slow_proxy(self):
        if self.proxy and self.proxy.get("_count", 0) > 9:
            return self.time_response() > self.LIMIT_TIME

    @staticmethod
    def get_timestamp():
        return int(datetime.utcnow().strftime("%s"))

    @staticmethod
    def get_score(proxy):
        return (proxy["_success"] - proxer.DIVIDER ** proxy["_fail"], -proxy["_timestamp"])

    def add(self, proxy):
        value = self.data.setdefault(proxy, {})
        value["addr"], value["port"] = proxy.split(":")
        value.setdefault("_success", 0)
        value.setdefault("_fail", 0)
        value.setdefault("_timestamp", self.get_timestamp())

    def get(self):
        self.proxy = None
        for k, v in self.data.items():
            if self.proxy is None or self.get_score(v) > self.get_score(self.proxy):
                self.proxy = v
                self.proxy_key = k
        if not self.proxy:
            raise ListProxiesEmpty()
        self.proxy["_timestamp"] = self.get_timestamp()
        return "%(addr)s:%(port)s" % self.proxy

    def ok(self, time_response=None):
        if self.proxy:
            self.proxy["_success"] += 1
            for i in range(self.LIMIT_FAIL):
                if self.proxy["_fail"] <= i:
                    break
                if self.proxy["_success"] > 2 ** (2 ** (self.LIMIT_FAIL - i)) + 10:
                    self.proxy["_fail"] = i
                    break
            if time_response:
                self.proxy.setdefault("_count", 0)
                self.proxy.setdefault("_total_time", 0.)
                self.proxy["_count"] += 1
                self.proxy["_total_time"] += time_response.total_seconds() + time_response.microseconds / 1000000.
                if self.is_slow_proxy():
                    raise SlowlyProxy("%s, %.3f" % (self.proxy["addr"], self.time_response()))

    def fail(self):
        if self.proxy:
            self.proxy["_success"] //= self.DIVIDER
            self.proxy["_fail"] += 1

    def time_response(self):
        if self.proxy and self.proxy.get("_count", 0):
            return self.proxy["_total_time"] / self.proxy["_count"]

    def __init__(self, file_name):
        self.file_name = file_name + ".json"
        self.load_data()
        with open(file_name, "r") as fo:
            for line in fo:
                line = line.strip()
                if not line:
                    continue
                self.add(line)
        self.proxy = None
        open(file_name, "w").close()

    def __exit__(self, *err):
        self.close()


class requester():
    cache_timeout = 10940
    caching = True
    assert_on_fail = True
    time_out = 4
    debug_output = True
    dir_cache = path.dirname(path.realpath(__file__)) + "/cache/"
    cookie_filename = path.abspath(path.realpath(__file__)) + ".cookie"
    last_page = None
    last_url = None
    ref_url = None
    time_sleep = 1e-1
    limit_file_cache = 200
    counter_file_cache = 0
    verify_word = None

    def print(self, *objs):
        if self.debug_output:
            print(datetime.utcnow(), *objs, file=stderr)

    def __init__(self,
                 proxy=bool(strtobool(environ.get('REQUESTER_PROXY', '0'))),
                 cookie_filename=None,
                 user_agent=None,
                 headers=None,
                 file_name_with_proxies=path.join(path.dirname(__file__), 'proxies.txt')):
        self.opened = None
        if cookie_filename:
            self.cookie_filename = cookie_filename
        if headers:
            self.headers = headers
        else:
            self.headers = [
                ('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'),
                ('Accept-Encoding', 'gzip, deflate'),
                ('Accept-Language', 'ru-ru,ru;q=0.8,en-us;q=0.5,en;q=0.3'),
                ('Connection', 'keep-alive'),
                (
                    'User-Agent',
                    'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:10.0.2) Gecko/20100101 Firefox/10.0.2'
                    if user_agent is None else user_agent
                )
            ]
        if self.cookie_filename:
            self.cookiejar = MozillaCookieJar(self.cookie_filename)
            if path.exists(self.cookie_filename):
                self.cookiejar.load()
        else:
            self.cookiejar = MozillaCookieJar()

        self.http_cookie_processor = urllib.request.HTTPCookieProcessor(self.cookiejar)
        self.opener = urllib.request.build_opener(self.http_cookie_processor)
        self.proxer = None
        if proxy:
            if proxy is True:
                if not file_name_with_proxies or not path.exists(file_name_with_proxies):
                    raise FileWithProxiesNotFound("ERROR: not found '%s' file" % file_name_with_proxies)
                self.proxer = proxer(file_name_with_proxies)
                proxy = self.proxer.get()
                self.print("[proxy]", proxy)
                time_response = self.proxer.time_response()
                if time_response:
                    self.print("[average time]", time_response)

            self.opener.add_handler(urllib.request.ProxyHandler({
                'http': proxy,
                'https': proxy,
            }))

        self._init_opener_headers = self.headers

    def get(
        self,
        url,
        post=None,
        caching=None,
        is_ref_url=True,
        md5_file_cache=None,
        time_out=None,
        headers=None,
        detect_charsets=True,
    ):
        prefix = "local-file:"
        if url.startswith(prefix):
            with open(url[len(prefix):], "r") as fo:
                page = fo.read().decode("utf8")
                self.last_page = page
            return page

        if not url.startswith('http') and self.last_url:
            url = urllib.parse.urljoin(self.last_url, url)
        if caching is None:
            caching = self.caching
        url = url.replace('&amp;', '&')
        url = url.replace(' ', '%20')

        makedirs(self.dir_cache, mode=0o777, exist_ok=True)

        if isinstance(post, dict):
            post = urllib.parse.urlencode(post).encode('utf-8')

        try:
            file_cache = ''.join((
                self.dir_cache,
                md5((md5_file_cache or url + (post if post else "")).encode()).hexdigest(),
                ("/" + url[url.find("//") + 2:].split("?", 2)[0]).replace("/", "_"),
                ".html",
            ))
        except Exception:
            file_cache = None

        caching = file_cache and caching and self.cache_timeout > 0

        from_cache = caching
        if caching:
            if not path.isfile(file_cache):
                from_cache = False
            else:
                diff_time = datetime.now() - datetime.fromtimestamp(path.getctime(file_cache))
                from_cache = diff_time.seconds < self.cache_timeout
        self.print("[cache]" if from_cache else "", url)
        self.error = None
        self.response = None
        if from_cache:
            with open(file_cache, "r") as f:
                page = f.read().encode('utf8')
        else:
            if self.time_sleep:
                v_time_sleep = min(1, abs(gauss(0, 1)) * self.time_sleep)
                sleep(v_time_sleep)
            if not headers:
                headers = {}
            if self.ref_url and 'Referer' not in headers:
                headers.update({"Referer": self.ref_url})
            if not self.last_url or urllib.parse.urlparse(self.last_url).netloc != urllib.parse.urlparse(url).netloc:
                self.opener.addheaders = self._init_opener_headers
            if headers:
                h = dict(self.opener.addheaders)
                h.update(headers)
                self.opener.addheaders = list(h.items())

            try:
                if headers:
                    request = urllib.request.Request(url, headers=headers)
                else:
                    request = url
                time_start = datetime.utcnow()
                response = self.opener.open(
                    request,
                    post if post else None,
                    timeout=time_out or self.time_out
                )
                if response.info().get("Content-Encoding", None) == "gzip":
                    buf = BytesIO(response.read())
                    page = GzipFile(fileobj=buf).read()
                else:
                    page = response.read()
                self.response = response
                self.time_response = datetime.utcnow() - time_start
                if self.verify_word and self.verify_word not in page:
                    raise NoVerifyWord("No verify word '%s', size page = %d" % (self.verify_word, len(page)))
            except Exception as err:
                self.error = err
                if self.assert_on_fail:
                    if self.proxer:
                        self.proxer.fail()
                    raise FailOnGetResponse(err)
                else:
                    traceback.print_exc()
                return

            try:
                if file_cache and caching:
                    cookie_write = True
                    if self.response.info().get("Content-Type").startswith("application/json"):
                        page = dumps(loads(page), indent=4)
                        cookie_write = False
                    if self.response.info().get("Content-Type").startswith("image/"):
                        cookie_write = False
                    with open(file_cache, "w") as f:
                        f.write(page.decode('utf8'))
                        if cookie_write:
                            f.write("\n\n" + dumps(self.get_cookies(), indent=4))
            except Exception:
                traceback.print_exc()
                self.print("[cache] ERROR: write to", file_cache)

            if self.proxer:
                if not self.error:
                    self.proxer.ok(self.time_response)
                else:
                    self.proxer.fail()

        if detect_charsets:
            matches = re.findall(r'charset=["\']?(?P<charset>[^"\'\s\.>;]{3,}\b)', str(page), re.IGNORECASE)
            if matches:
                charsets = [c.lower() for c in matches]
                if len(charsets) > 1 and len(set(charsets)) > 1:
                    self.print(f'[WARNING] set multi charset values: {charsets}')
                charset = charsets[-1].lower()
            else:
                charset = 'utf-8'
            charset_detect = chardet.detect(page)
            if charset_detect and charset_detect['confidence'] > 0.98:
                charset = charset_detect['encoding']
            if charset in ('utf-8', 'utf8'):
                page = page.decode('utf-8', 'replace')
            elif charset in ('windows-1251', 'cp1251'):
                page = page.decode('cp1251', 'replace')
            else:
                page = page.decode(charset, 'replace')

        self.last_page = page
        self.last_url = self.response.geturl() if self.response else url
        if is_ref_url:
            self.ref_url = self.last_url
        self.file_cache_clear()
        return page

    def get_link_by_text(self, text, page=None):
        if page is None:
            page = self.last_page
        match = re.search(
            r"""
            <a[^>]*href="(?P<href>[^"]*)"[^>]*>\s*
                (?:</?[^a][^>]*>\s*)*
                %s
            """ % text.replace(" ", r"\s"),
            page,
            re.VERBOSE
        )
        if not match:
            return
        return match.group("href")

    def get_link_by_text_and_go_if_exist(self, text):
        url = self.get_link_by_text(text)
        if url:
            self.get(url)
        return self.last_page

    def form(self, page=None, action='', limit=2, fid=None, selectors=()):
        if page is None:
            page = self.last_page
        selectors = list(selectors) + [
            '''method=["'](?P<method>post|get)"''',
            f'''action=["'](?P<url>[^"']*{action}[^"']*)["']''',
        ]
        if fid is not None:
            selectors.append(f'id="{fid}"')
        selector = '|[^>]*'.join(selectors)
        regex = f'''
            <form([^>]*{selector}){{{limit}}}[^>]*>
            .*?
            </form>
        '''
        match = re.search(regex, page, re.DOTALL | re.VERBOSE | re.IGNORECASE)
        if not match:
            return None
        page = match.group()
        result = match.groupdict()
        post = {}
        fields = re.finditer(
            r'''
            (?:
                type=["'](?P<type>[^"']*)["']\s*|
                value=["'](?P<value>[^"']*)["']\s*|
                name=["'](?P<name>[^"']*)["']\s*|
                [-a-z]+=["'][^"']*["']\s*|
                (?P<checked>checked)\s*
            ){2,}''',
            page,
            re.VERBOSE | re.IGNORECASE
        )

        unchecked = []
        for field in fields:
            field = field.groupdict()
            if field["name"] is None or field["value"] is None:
                continue
            if field["type"] == "checkbox" and field["checked"] is None:
                unchecked.append(field)
            else:
                post[field['name']] = field['value'].encode('utf8')

        fields = re.finditer(r'''<select[^>]*name="(?P<name>[^"]*)"[^>]*>''', page, re.VERBOSE)
        for field in fields:
            post[field.group('name')] = ''

        result['post'] = post
        if unchecked:
            result['unchecked'] = unchecked
        return result

    def submit_form(self, data, *args, url=None, **kwargs):
        form = self.form(*args, **kwargs)
        form['post'].update(data)
        data = urllib.parse.urlencode(form['post']).encode('utf-8')
        url = url or form['url']
        ret = {
            'get': lambda: self.get(urllib.parse.urljoin(url, f'?{data}')),
            'post': lambda: self.get(url, data),
        }[form['method'].lower()]()
        return ret

    def file_cache_clear(self):
        if self.limit_file_cache and self.counter_file_cache % self.limit_file_cache == 0:
            file_list = []
            for file_cache in listdir(self.dir_cache):
                stat_file = stat(self.dir_cache + file_cache)
                file_list.append((stat_file.st_atime, file_cache))
            file_list.sort(reverse=True)
            for atime, file_cache in file_list[self.limit_file_cache:]:
                remove(self.dir_cache + file_cache)
        self.counter_file_cache += 1

    def get_raw_cookies(self):
        for c in self.cookiejar:
            yield c

    def get_cookies(self):
        return dict(((i.name, i.value) for i in self.cookiejar))

    def get_cookie(self, name):
        return self.get_cookies().get(name, None)

    def set_cookie(self, name, value):
        for c in self.cookiejar:
            if c.name == name:
                c.value = value
                self.cookiejar.set_cookie(c)
                self.print("[setcookie]", name)
                break

    @staticmethod
    def rand_string(length):
        a = ascii_letters + digits
        return ''.join([choice(a) for i in range(length)])

    def save_cookie(self):
        if self.cookie_filename:
            try:
                self.cookiejar.save()
            except Exception:
                pass

    def close(self):
        if self.proxer:
            self.proxer.save_data()
        self.save_cookie()

    def __enter__(self):
        if self.opened is not True:
            self.opened = True
        return self

    def __exit__(self, *err):
        if self.opened is not False:
            self.opened = False
            self.close()

    def __del__(self):
        if not isdir(self.dir_cache):
            return

        for file_cache in listdir(self.dir_cache):
            diff_time = (datetime.now() - datetime.fromtimestamp(getctime(self.dir_cache + file_cache)))
            if diff_time.seconds >= self.cache_timeout:
                remove(self.dir_cache + file_cache)


if __name__ == "__main__":
    headers = [
        ('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'),
        ('Accept-Encoding', 'gzip,deflate,sdch'),
        ('Accept-Language', 'ru-RU,ru;q=0.8,en-US;q=0.6,en;q=0.4'),
        ('Proxy-Connection', 'keep-alive'),
        ('User-Agent',
         'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) '
         'Ubuntu Chromium/37.0.2062.120 Chrome/37.0.2062.120 Safari/537.36'),
    ]
    req = requester(headers=headers)
    req.caching = False
    req.time_sleep = 1
    req.get("http://opencup.ru")
    req.get("http://clist.by")
    del req
